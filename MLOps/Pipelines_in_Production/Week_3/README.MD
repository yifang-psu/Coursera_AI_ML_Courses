### Week 3: High-Performance Modeling

* **What is this week's course about?**
  * Implement distributed processing and parallelism techniques to make the most of your computational resources for training your models efficiently.

* **Learning Objectives:**
  * Identify the rise in computational requirements with current network architectures
  * Select techniques to optimize the usage of computational resources to best serve your model needs
  * Carry out high-performance data ingestion to reduce hardware accelerators idling time
  * Distinguish between data and model parallelism to train your models in the most efficient way
  * Implement knowledge distillation to reduce models that capture complex relationships among features so that they fit in constrained deployment environments

* **Reading List:**
  * [Distributed training](https://www.tensorflow.org/guide/distributed_training)
  * [Data parallelism](https://arxiv.org/abs/1806.03377)
  * [Pipeline paralleism](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html)
  * [GPipe](https://arxiv.org/abs/1811.06965)
  * [Knowledge distillation](https://arxiv.org/pdf/1503.02531.pdf)
  * [Q&A case study](https://arxiv.org/pdf/1910.08381.pdf)
  
* **Lab Exercise:**
  * [Distributed Strategies with TF and Keras](https://github.com/yifang-psu/Coursera_AI_ML_Courses/blob/main/MLOps/Pipelines_in_Production/Week_3/Lab/C3_W3_Lab_1_Distributed_Training.ipynb)
  * [Knowledge Distillation]()

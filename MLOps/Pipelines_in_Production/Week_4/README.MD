### Week 4: Model Analysis

* **What is this week's course about?**
  * Use model performance analysis to debug and remediate your model and measure robustness, fairness, and stability.

* **Learning Objectives:**
  * Determine and analyze model performance through metric based black box evaluation
  * Carry out model introspection to reflect on how different components of a model affect performance
  * Integrate TensorFlow Model Analysis to your production pipeline
  * Use different slices of data to gain further insight on your models pitfalls and shortcomings
  * Establish tools and analysis to fix and debug your models
  * Determine how to monitor and protect your models against random and adversarial attacks
  * Check model fairness and evaluate model performance against commonly used fairness metrics
  * Provide ways to continuously evaluate and monitor your models in production for ensuring stable performance of ML applications

* **Reading List:**
  * [TensorBoard](https://blog.tensorflow.org/2019/12/introducing-tensorboarddev-new-way-to.html)
  * [TensorFlow Model Analysis (TFMA)](https://blog.tensorflow.org/2018/03/introducing-tensorflow-model-analysis.html)
  * [TFMA architecture](https://www.tensorflow.org/tfx/model_analysis/architecture)
  * [Sensitivity Analysis and Adversarial Attacks](https://arxiv.org/abs/1412.6572)
  * Model Remediation and Fairness
    * [Fairness](https://www.tensorflow.org/responsible_ai/fairness_indicators/guide)
    * [Learning fair representations](https://arxiv.org/pdf/1904.13341.pdf)
    * [Fairness-aware Machine Learning library](https://github.com/cosmicBboy/themis-ml)
    * [AI 360 open source model fairness library](http://aif360.mybluemix.net/)
    * [Model remediation](https://www.tensorflow.org/responsible_ai/model_remediation)
    * [Model cards](https://modelcards.withgoogle.com/about)
  * Continuous Evaluation and Mornitoring
    * [Instrumentation, Observability & Monitoring of Machine Learning Models](https://www.infoq.com/presentations/instrumentation-observability-monitoring-ml/)
    * [Monitoring Machine Learning Models in Production - A Comprehensive Guide](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/)
    * [Concept Drift detection for Unsupervised Learning](https://arxiv.org/pdf/1704.00023.pdf)
    * [Google Cloud](https://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation)
    * [Amazon SageMaker](https://aws.amazon.com/sagemaker/model-monitor/)
    * [Microsoft Azure](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets?tabs=python)
  
* **Lab Exercise:**
  * [Tensorflow Model Analysis]()
  * [Model Analysis with TFX Evaluator]()
  * [Faireness Indicators]()


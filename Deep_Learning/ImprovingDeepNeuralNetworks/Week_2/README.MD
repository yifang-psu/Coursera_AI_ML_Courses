### Week 2 Learning Objectives 

* Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
* Use random minibatches to accelerate convergence and improve optimization
* Describe the benefits of learning rate decay and apply it to your optimization

* **Programming Assignements**:
  * [Optimization Methods](https://github.com/yifang-psu/Coursera_AI_ML_Courses/blob/main/Deep_Learning/ImprovingDeepNeuralNetworks/Week_1/Initialization.ipynb):
    * You'll gain skills with some more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. 
